{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Quick Response Quality Evaluation\n",
    "\n",
    "This notebook tests the`QuickResponseQualityLabeler` functionality locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from ypl.backend.llm.chat import ModelInfo, get_chat_model, ChatProvider\n",
    "from ypl.backend.llm.judge import QuickResponseQualityLabeler\n",
    "from ypl.backend.prompts import JUDGE_QUICK_RESPONSE_QUALITY_SYSTEM_PROMPT, JUDGE_QUICK_RESPONSE_QUALITY_USER_PROMPT\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize LLM\n",
    "model_name = \"gpt-4o-mini\"  # Can be modified as needed\n",
    "llm = get_chat_model(ModelInfo(\n",
    "    model=model_name,\n",
    "    provider=ChatProvider.OPENAI,\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    "))\n",
    "\n",
    "# Initialize labeler\n",
    "labeler = QuickResponseQualityLabeler(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence warnings and info\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configure logging to silence httpx\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt:\n",
      "\n",
      "You are an AI assistant tasked with evaluating the quality of short Twitter-like AI responses to prompts (and conversation history if available).\n",
      "\n",
      "Consider these factors:\n",
      "- Accuracy: Is the response factually correct?\n",
      "- Brevity: Is the response concise without any extraneous words? (Should be â‰¤140 characters)\n",
      "- Formatting: Is the response plain text without formatting, markdown, or newlines?\n",
      "- Completeness: Is the response complete and not truncated mid-sentence?\n",
      "- Relevance: Does the response address the user's prompt?\n",
      "- Tone: Is the response appropriate and friendly?\n",
      "\n",
      "Special case for [NULL] responses: a [NULL] response represents a refusal to answer the prompt, since a short response is inadequate for the prompt.\n",
      "- POOR: If the prompt could be answered briefly but wasn't.\n",
      "- ACCEPTABLE: If it's unclear whether a brief answer was possible.\n",
      "- EXCELLENT: If a long answer is required and the AI correctly refuses.\n",
      "\n",
      "Return format: Respond with one of these ratings:\n",
      "1 - POOR quality\n",
      "2 - ACCEPTABLE quality\n",
      "3 - EXCELLENT quality\n",
      "\n",
      "Return the number rating only.\n",
      "\n",
      "\n",
      "User Prompt Template:\n",
      "\n",
      "Conversation History:\n",
      "{chat_history}\n",
      "\n",
      "Prompt:\n",
      "{user_prompt}\n",
      "\n",
      "Response to evaluate:\n",
      "{response}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the prompts to verify they're correct\n",
    "print(\"System Prompt:\")\n",
    "print(JUDGE_QUICK_RESPONSE_QUALITY_SYSTEM_PROMPT)\n",
    "print(\"\\nUser Prompt Template:\")\n",
    "print(JUDGE_QUICK_RESPONSE_QUALITY_USER_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Input:\n",
      "{'user_prompt': \"What's the capital of France?\", 'response': 'Paris', 'chat_history': 'No previous conversation'}\n",
      "\n",
      "Rating (1=poor, 2=acceptable, 3=excellent):\n",
      "excellent\n"
     ]
    }
   ],
   "source": [
    "# Test single case with debug info\n",
    "test_case = {\n",
    "    \"prompt\": \"What's the capital of France?\",\n",
    "    \"response\": \"Paris\",\n",
    "    \"chat_history\": []\n",
    "}\n",
    "\n",
    "# Debug the input preparation\n",
    "prepared_input = labeler._prepare_input((test_case[\"prompt\"], test_case[\"response\"], test_case[\"chat_history\"]))\n",
    "print(\"Prepared Input:\")\n",
    "print(prepared_input)\n",
    "\n",
    "# Test evaluation\n",
    "rating = await labeler.alabel((test_case[\"prompt\"], test_case[\"response\"], test_case[\"chat_history\"]))\n",
    "print(\"\\nRating (1=poor, 2=acceptable, 3=excellent):\")\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing case with prompt: What's the capital of France?\n",
      "Response: Paris\n",
      "Expected rating: excellent\n",
      "Actual rating: excellent\n",
      "\n",
      "Testing case with prompt: What's the capital of France?\n",
      "Response: The capital of France is Paris.\n",
      "Expected rating: excellent\n",
      "Actual rating: excellent\n",
      "\n",
      "Testing case with prompt: Write a long essay about AI\n",
      "Response: [NULL]\n",
      "Expected rating: excellent\n",
      "Actual rating: excellent\n",
      "\n",
      "Testing case with prompt: How's the weather?\n",
      "Response: I don't have access to real-time weather data.\n",
      "Expected rating: acceptable\n",
      "Actual rating: acceptable\n",
      "\n",
      "Testing case with prompt: What's 2+2?\n",
      "Response: Let me explain step by step: First, we take 2 and add another 2 to it. This basic arithmetic operation results in the sum of 4.\n",
      "Expected rating: poor\n",
      "Actual rating: poor\n",
      "\n",
      "Testing case with prompt: Can you explain quantum mechanics?\n",
      "Response: Quantum mechanics is a fundamental theory in physics that describes the behavior of matter and energy at the atomic and subatomic scale. It deals with phenomena like superposition, entanglement, and wave-particle duality. The mathematical framework...\n",
      "Expected rating: poor\n",
      "Actual rating: poor\n",
      "\n",
      "Testing case with prompt: What's the best programming language?\n",
      "Response: Ugh, not this question again! Only noobs ask this. Real programmers know it depends on the use case.\n",
      "Expected rating: poor\n",
      "Actual rating: poor\n",
      "\n",
      "Testing case with prompt: Can you explain how a car engine works?\n",
      "Response: A car engine converts fuel into\n",
      "Expected rating: poor\n",
      "Actual rating: poor\n",
      "\n",
      "Testing case with prompt: What are the key features of Python?\n",
      "Response: ### Key Python Features\n",
      "- Easy to read syntax\n",
      "- Dynamic typing\n",
      "- Extensive libraries\n",
      "\n",
      "Perfect for beginners and experts alike.\n",
      "Expected rating: poor\n",
      "Actual rating: poor\n",
      "\n",
      "Testing case with prompt: what is the super long word by Mary Poppins' son\n",
      "Response: Supercalifragilisticexpialidocious.\n",
      "Expected rating: poor\n",
      "Actual rating: poor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test more cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"prompt\": \"What's the capital of France?\",\n",
    "        \"response\": \"Paris\",\n",
    "        \"chat_history\": [],\n",
    "        \"expected\": \"excellent\"  # Short, accurate, relevant\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What's the capital of France?\",\n",
    "        \"response\": \"The capital of France is Paris.\",\n",
    "        \"chat_history\": [],\n",
    "        \"expected\": \"excellent\"  # Short, accurate, relevant\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Write a long essay about AI\",\n",
    "        \"response\": \"[NULL]\",\n",
    "        \"chat_history\": [],\n",
    "        \"expected\": \"excellent\"  # Correctly refuses long response\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"How's the weather?\",\n",
    "        \"response\": \"I don't have access to real-time weather data.\",\n",
    "        \"chat_history\": [],\n",
    "        \"expected\": \"acceptable\"  # Clear response but could be more concise\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What's 2+2?\",\n",
    "        \"response\": \"Let me explain step by step: First, we take 2 and add another 2 to it. This basic arithmetic operation results in the sum of 4.\",\n",
    "        \"chat_history\": [],\n",
    "        \"expected\": \"poor\"  # Too verbose for a simple question\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Can you explain quantum mechanics?\",\n",
    "        \"response\": \"Quantum mechanics is a fundamental theory in physics that describes the behavior of matter and energy at the atomic and subatomic scale. It deals with phenomena like superposition, entanglement, and wave-particle duality. The mathematical framework...\",\n",
    "        \"chat_history\": [],\n",
    "        \"expected\": \"poor\"  # Too verbose, should have refused\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What's the best programming language?\",\n",
    "        \"response\": \"Ugh, not this question again! Only noobs ask this. Real programmers know it depends on the use case.\",\n",
    "        \"chat_history\": [],\n",
    "        \"expected\": \"poor\"  # Unhelpful response\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"Can you explain how a car engine works?\",\n",
    "        \"response\": \"A car engine converts fuel into\",\n",
    "        \"chat_history\": [],\n",
    "        \"expected\": \"poor\"  # Truncated mid-sentence, should have given complete response or refused\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"What are the key features of Python?\",\n",
    "        \"response\": \"### Key Python Features\\n- Easy to read syntax\\n- Dynamic typing\\n- Extensive libraries\\n\\nPerfect for beginners and experts alike.\",\n",
    "        \"chat_history\": [],\n",
    "        \"expected\": \"poor\"  # Markdown formatting inappropriate for quick response\n",
    "    },\n",
    "    {\n",
    "        \"prompt\": \"what is the super long word by Mary Poppins' son\",\n",
    "        \"response\": \"Supercalifragilisticexpialidocious.\",\n",
    "        \"chat_history\": [],\n",
    "        \"expected\": \"poor\"  # Incorrect, she doesn't have a son\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test with debug info\n",
    "async def test_cases_with_debug():\n",
    "    for case in test_cases:\n",
    "        print(f\"Testing case with prompt: {case['prompt']}\")\n",
    "        print(f\"Response: {case['response']}\")\n",
    "        print(f\"Expected rating: {case['expected']}\")\n",
    "        \n",
    "        rating = await labeler.alabel((case[\"prompt\"], case[\"response\"], case[\"chat_history\"]))\n",
    "        print(f\"Actual rating: {rating}\\n\")\n",
    "\n",
    "await test_cases_with_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Input:\n",
      "{'user_prompt': 'What about the second one?', 'response': 'K2', 'chat_history': [{'role': 'user', 'content': \"What's the tallest mountain in the world?\"}, {'role': 'assistant', 'content': 'Mount Everest'}, {'role': 'user', 'content': 'What about the second one?'}]}\n",
      "\n",
      "Rating (1=poor, 2=acceptable, 3=excellent):\n",
      "excellent\n"
     ]
    }
   ],
   "source": [
    "# Test with chat history\n",
    "test_with_history = {\n",
    "    \"prompt\": \"What about the second one?\",\n",
    "    \"response\": \"K2\",\n",
    "    \"chat_history\": [\n",
    "        {\"role\": \"user\", \"content\": \"What's the tallest mountain in the world?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Mount Everest\"},\n",
    "        {\"role\": \"user\", \"content\": \"What about the second one?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Debug the input preparation\n",
    "prepared_input = labeler._prepare_input((\n",
    "    test_with_history[\"prompt\"],\n",
    "    test_with_history[\"response\"],\n",
    "    test_with_history[\"chat_history\"]\n",
    "))\n",
    "print(\"Prepared Input:\")\n",
    "print(prepared_input)\n",
    "\n",
    "# Test evaluation\n",
    "rating = await labeler.alabel((\n",
    "    test_with_history[\"prompt\"],\n",
    "    test_with_history[\"response\"],\n",
    "    test_with_history[\"chat_history\"]\n",
    "))\n",
    "print(\"\\nRating (1=poor, 2=acceptable, 3=excellent):\")\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared Input:\n",
      "{'user_prompt': 'What about the second one?', 'response': 'K2', 'chat_history': [{'role': 'user', 'content': 'What is the largest animal in the world?'}, {'role': 'assistant', 'content': 'The blue whale'}, {'role': 'user', 'content': 'What about the second one?'}]}\n",
      "\n",
      "Rating (1=poor, 2=acceptable, 3=excellent):\n",
      "poor\n"
     ]
    }
   ],
   "source": [
    "# Test with chat history\n",
    "test_with_history = {\n",
    "    \"prompt\": \"What about the second one?\",\n",
    "    \"response\": \"K2\",\n",
    "    \"chat_history\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the largest animal in the world?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"The blue whale\"},\n",
    "        {\"role\": \"user\", \"content\": \"What about the second one?\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Debug the input preparation\n",
    "prepared_input = labeler._prepare_input((\n",
    "    test_with_history[\"prompt\"],\n",
    "    test_with_history[\"response\"],\n",
    "    test_with_history[\"chat_history\"]\n",
    "))\n",
    "print(\"Prepared Input:\")\n",
    "print(prepared_input)\n",
    "\n",
    "# Test evaluation\n",
    "rating = await labeler.alabel((\n",
    "    test_with_history[\"prompt\"],\n",
    "    test_with_history[\"response\"],\n",
    "    test_with_history[\"chat_history\"]\n",
    "))\n",
    "print(\"\\nRating (1=poor, 2=acceptable, 3=excellent):\")\n",
    "print(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
