{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of tags assigned to user prompts\n",
    "\n",
    "This notebook collects a dataset of user prompts, assigns descriptive tags to them using an LLM-judge, and studies the aggregates of the tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source of data\n",
    "```sql\n",
    "select \n",
    "  u.user_id, u.email, u.country_code,\n",
    "  cm.message_id, cm.message_type, cm.content, cm.created_at, cm.language_code, cm.annotations, cm.completion_status,\n",
    "  t.turn_id, t.sequence_id as turn_sequence_id,\n",
    "  tq.prompt_difficulty, tq.prompt_is_safe,\n",
    "  lm.internal_name as lm_internal_name,\n",
    "  c.name as category_name\n",
    "from chat_messages cm\n",
    "join turns t on t.turn_id = cm.turn_id \n",
    "join users u on t.creator_user_id = u.user_id \n",
    "left join turn_qualities tq on tq.turn_id = t.turn_id \n",
    "left join categories c on c.category_id = cm.category_id \n",
    "left join language_models lm on lm.language_model_id = cm.assistant_language_model_id \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json\n",
    "from collections import Counter\n",
    "from scipy.stats import chi2_contingency, power_divergence\n",
    "import numpy as np\n",
    "\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import ypl.db.all_models\n",
    "from ypl.backend.llm.labeler import InputType, LLMLabeler, OnErrorBehavior, OutputType\n",
    "from ypl.backend.llm.judge import TruncatedPromptLabeler\n",
    "from ypl.backend.config import settings\n",
    "from ypl.backend.llm.constants import ChatProvider\n",
    "from ypl.backend.llm.model_data_type import ModelInfo\n",
    "from ypl.backend.llm.vendor_langchain_adapter import GeminiLangChainAdapter\n",
    "logging.getLogger().setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use a simple, cheap model for this.\n",
    "GEMINI_15_FLASH_LLM = GeminiLangChainAdapter(\n",
    "    model_info=ModelInfo(\n",
    "        provider=ChatProvider.GOOGLE,\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        api_key=settings.GOOGLE_API_KEY,\n",
    "    ),\n",
    "    model_config_=dict(\n",
    "        project_id=settings.GCP_PROJECT_ID,\n",
    "        region=settings.GCP_REGION,\n",
    "        temperature=0.0,\n",
    "        max_output_tokens=256,\n",
    "        top_k=1,\n",
    "    ),\n",
    ")\n",
    "\n",
    "JUDGE_PROMPT = \"\"\"\n",
    "You are a model that assigns \"tags\" summarizing the user's prompt, as a json with two arrays of strings:\n",
    "- \"high_level_tags\": an array of high-levels tags that describe the prompt, such as \"weather\", \"news\", \"code\", etc.\n",
    "- \"detailed_tags\": an array of detailed tags that describe the prompt.\n",
    "tags are ideally 1-3 words, and should be short, descriptive, and relevant. Do not include any markup, just the json array.\n",
    "Example output:\n",
    "{{\"high_level_tags\": [\"weather\"], \"detailed_tags\": [\"climate patterns\", \"temperature\", \"rainfall\"]}}\n",
    "\n",
    "Assign high_level_tags and detailed_tags to the following prompt:\n",
    "\n",
    "{prompt}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class PromptLabeler(TruncatedPromptLabeler[dict[str, list[str]]]):\n",
    "    cached = True\n",
    "\n",
    "    def _prepare_llm(self, llm: BaseChatModel) -> BaseChatModel:\n",
    "        cp_template = ChatPromptTemplate.from_messages([(\"human\", JUDGE_PROMPT)])\n",
    "        return cp_template | llm  # type: ignore\n",
    "\n",
    "    def _parse_output(self, output: BaseMessage) -> dict[str, list[str]]:\n",
    "        content = output.content.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        return json.loads(content)\n",
    "\n",
    "    @property\n",
    "    def error_value(self) -> list[str]:\n",
    "        return []\n",
    "\n",
    "labeler = PromptLabeler(GEMINI_15_FLASH_LLM, max_prompt_len=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data, which comes from the SQL query above.\n",
    "df_all = pd.read_csv('messages.csv')\n",
    "\n",
    "# Exclude yuppsters.\n",
    "df = df_all[~df_all.email.str.contains(\"yupp.ai\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Entire dataset ---\n",
      "Messages: 95769\n",
      "message_type:\n",
      "  ASSISTANT_MESSAGE: 49987\n",
      "  USER_MESSAGE: 22916\n",
      "  QUICK_RESPONSE_MESSAGE: 22866\n",
      "\n",
      "category_name:\n",
      "  Factual: 5852\n",
      "  Other: 3776\n",
      "  Code: 2891\n",
      "  Creative Writing: 1764\n",
      "  Education: 1480\n",
      "\n",
      "language_code:\n",
      "  EN: 91702\n",
      "  FR: 671\n",
      "  ID: 491\n",
      "  ES: 312\n",
      "  IT: 246\n",
      "\n",
      "prompt_difficulty:\n",
      "  7.0: 20675\n",
      "  8.0: 15430\n",
      "  3.0: 11060\n",
      "  4.0: 9371\n",
      "  1.0: 8561\n",
      "\n",
      "email:\n",
      "  gilad@yupp.ai: 6664\n",
      "  mnn@yogins.com: 4385\n",
      "  bhanu@yupp.ai: 3941\n",
      "  w@yupp.ai: 3455\n",
      "  minqi@yupp.ai: 3312\n",
      "\n",
      "--- Excluding yuppsters ---\n",
      "Messages: 56991\n",
      "message_type:\n",
      "  ASSISTANT_MESSAGE: 30041\n",
      "  USER_MESSAGE: 13485\n",
      "  QUICK_RESPONSE_MESSAGE: 13465\n",
      "\n",
      "category_name:\n",
      "  Factual: 3126\n",
      "  Other: 1802\n",
      "  Code: 1778\n",
      "  Creative Writing: 1036\n",
      "  Education: 1033\n",
      "\n",
      "language_code:\n",
      "  EN: 54323\n",
      "  FR: 546\n",
      "  ID: 478\n",
      "  ES: 221\n",
      "  LT: 183\n",
      "\n",
      "prompt_difficulty:\n",
      "  7.0: 15867\n",
      "  8.0: 13307\n",
      "  4.0: 6428\n",
      "  3.0: 5981\n",
      "  9.0: 3776\n",
      "\n",
      "email:\n",
      "  mnn@yogins.com: 4385\n",
      "  babak.hamadani@gmail.com: 1862\n",
      "  tysreddy@gmail.com: 1711\n",
      "  adithya.vadapalli2604@gmail.com: 1266\n",
      "  cmay3249@gmail.com: 1123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_stats(df: pd.DataFrame):\n",
    "    def print_counts(df: pd.DataFrame, col: str):\n",
    "        print(f\"{col}:\")\n",
    "        for v, c in df[col].value_counts().head().items():\n",
    "            print(f\"  {v}: {c}\")\n",
    "        print()\n",
    "    print(f\"Messages: {len(df)}\")\n",
    "    print_counts(df, \"message_type\")\n",
    "    print_counts(df, \"category_name\")\n",
    "    print_counts(df, \"language_code\")\n",
    "    print_counts(df, \"prompt_difficulty\")\n",
    "    print_counts(df, \"email\")\n",
    "    \n",
    "print(\"--- Entire dataset ---\")\n",
    "print_stats(df_all)\n",
    "\n",
    "print(\"--- Excluding yuppsters ---\")\n",
    "print_stats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag to skip the LLM-based tagging step, if it was already run and stored locally.\n",
    "PRE_CALC_PROMPTS = True\n",
    "\n",
    "if not PRE_CALC_PROMPTS:\n",
    "    prompts = df[\n",
    "        (df.message_type == \"USER_MESSAGE\") & (df.content.str.len() > 5)\n",
    "    ]\n",
    "\n",
    "    res = await labeler.abatch_label(prompts.content.tolist(), num_parallel=4)\n",
    "\n",
    "    def get_tags(result, col):\n",
    "        return result.get(col, []) if isinstance(result, dict) else []\n",
    "\n",
    "    prompts[\"high_level_tags\"] = [get_tags(x, \"high_level_tags\") for x in res]\n",
    "    prompts[\"detailed_tags\"] = [get_tags(x, \"detailed_tags\") for x in res]\n",
    "\n",
    "    with open(\"prompts.csv\", \"w\") as f:\n",
    "        prompts.to_csv(f, index=False)\n",
    "\n",
    "prompts = pd.read_csv(\"prompts.csv\")\n",
    "prompts[\"high_level_tags\"] = prompts[\"high_level_tags\"].apply(eval)\n",
    "prompts[\"detailed_tags\"] = prompts[\"detailed_tags\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_tags(df: pd.DataFrame, col: str):\n",
    "    return [tag for tags in df[col] for tag in tags]\n",
    "\n",
    "all_tags = {\n",
    "    col: flatten_tags(prompts, col) for col in [\"high_level_tags\", \"detailed_tags\"]\n",
    "}\n",
    "all_counts = {\n",
    "    col: Counter(all_tags[col]) for col in all_tags\n",
    "}\n",
    "\n",
    "def analyze_tags(df: pd.DataFrame, tag_col: str):\n",
    "    # Calculate tag counts for df.\n",
    "    tags = flatten_tags(df, tag_col)\n",
    "    counts = Counter(tags)\n",
    "    \n",
    "    # Display the most common tags.\n",
    "    common_tags = dict(counts.most_common(30))\n",
    "    \n",
    "    # Totals in the subset and overall dataset.\n",
    "    total = sum(counts.values())\n",
    "    total_all = sum(all_counts[tag_col].values())\n",
    "    total_rest = total_all - total\n",
    "    \n",
    "    # Dictionaries to store divergence scores.\n",
    "    chi2_scores = {}\n",
    "    llr_chi2_scores = {}\n",
    "    \n",
    "    # Consider all tags present either in the overall counts or in the df.\n",
    "    tokens = set(all_counts[tag_col].keys()).union(counts.keys())\n",
    "    for token in tokens:\n",
    "        count = counts.get(token, 0)\n",
    "        count_overall = all_counts[tag_col].get(token, 0)\n",
    "        count_rest = count_overall - count\n",
    "        \n",
    "        # Build 2x2 contingency table.\n",
    "        table = [\n",
    "            [count, count_rest],\n",
    "            [total - count, total_rest - count_rest]\n",
    "        ]\n",
    "        \n",
    "        # Pearson's chi-square test.\n",
    "        try:\n",
    "            chi2, p_value, dof, expected = chi2_contingency(table, correction=False)\n",
    "            chi2_scores[token] = (chi2, p_value, dof, expected)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "    # Sort tags by divergence scores (higher score = more differing).\n",
    "    sorted_chi2 = sorted(chi2_scores.items(), key=lambda x: x[1][0], reverse=True)\n",
    "    \n",
    "    # Print the top diverging tags.\n",
    "    for token, (score, p_value, dof, expected) in sorted_chi2[:30]:\n",
    "        obs = counts.get(token, 0)\n",
    "        overall = all_counts[tag_col].get(token,0)\n",
    "        obs_pct = obs / total\n",
    "        overall_pct = overall / total_all\n",
    "        # Format with fixed width using ljust() for consistent alignment\n",
    "        token_str = f\"{token}\".ljust(25)\n",
    "        stats_str = f\"chi2={score:.2f}\".ljust(15)\n",
    "        counts_str = f\"obs={obs} ({obs_pct:.3%}) overall={overall} ({overall_pct:.3%})\"\n",
    "        # print(f\"  {token_str}{stats_str}{counts_str}\")\n",
    "    return common_tags, {token: v[0] for token, v in sorted_chi2[:30]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opinion...Other...Factual...Education...Math...Analysis...Creative Writing...Summarization...Comparison...Multilingual...Code...Advice...Reasoning...Entertainment...nan...\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for category in prompts.category_name.unique():\n",
    "    category_df = prompts[prompts.category_name == category]\n",
    "    top_detailed_tags, chi2_detailed_tags = analyze_tags(category_df, \"detailed_tags\")\n",
    "    top_high_level_tags, chi2_high_level_tags = analyze_tags(category_df, \"high_level_tags\")\n",
    "    data[category] = {\n",
    "        \"top_detailed_tags\": top_detailed_tags,\n",
    "        \"chi2_detailed_tags\": chi2_detailed_tags,\n",
    "        \"top_high_level_tags\": top_high_level_tags,\n",
    "        \"chi2_high_level_tags\": chi2_high_level_tags,\n",
    "    }\n",
    "    print(category, end=\"...\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CSVs from the chi2_detailed_tags data, for loading to excel\n",
    "detailed_tags = {\n",
    "    category: list(data[category]['chi2_detailed_tags'].keys())\n",
    "    for category in data.keys()\n",
    "    if not pd.isna(category)\n",
    "}\n",
    "detailed_tags_df = pd.DataFrame(detailed_tags)\n",
    "detailed_tags_df.to_csv(\"detailed_tags.csv\", index=False)\n",
    "\n",
    "high_level_tags = {\n",
    "    category: list(data[category]['chi2_high_level_tags'].keys())\n",
    "    for category in data.keys()\n",
    "    if not pd.isna(category)\n",
    "}\n",
    "high_level_tags_df = pd.DataFrame(high_level_tags)\n",
    "high_level_tags_df.to_csv(\"high_level_tags.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ys-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
