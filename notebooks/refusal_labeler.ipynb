{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refusal response labeler\n",
    "\n",
    "Tests an LLM-judge refusal detection model against a simple regex-based model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.4.1 available.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from typing import Any\n",
    "from sqlalchemy import func\n",
    "from sqlmodel import Session, select\n",
    "import logging\n",
    "import httpx\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from ypl.backend.llm.labeler import LLMLabeler, OnErrorBehavior\n",
    "from ypl.backend.config import settings\n",
    "from ypl.backend.db import get_engine\n",
    "from ypl.db.chats import Chat, ChatMessage, LanguageCode, MessageType, Turn, TurnQuality\n",
    "from ypl.backend.llm.chat import ModelInfo, get_chat_model\n",
    "from ypl.backend.llm.constants import ChatProvider\n",
    "from ypl.backend.llm.judge import ResponseRefusalLabeler\n",
    "# Make the HTTP requests less chatty.\n",
    "logging.getLogger(\"httpx\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_llm = get_chat_model(\n",
    "    ModelInfo(\n",
    "        provider=ChatProvider.OPENAI,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        api_key=settings.OPENAI_API_KEY,\n",
    "    ),\n",
    "    temperature=0.0,\n",
    ")\n",
    "labeler = ResponseRefusalLabeler(judge_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1320 prompt-response pairs\n"
     ]
    }
   ],
   "source": [
    "# Get turns to label.\n",
    "\n",
    "num_parallel = 12\n",
    "\n",
    "max_num_turns = 1000\n",
    "prompts_responses = []\n",
    "message_ids = []\n",
    "\n",
    "with Session(get_engine()) as session:\n",
    "    turn_ids = session.exec(\n",
    "        select(Turn.turn_id)\n",
    "        .order_by(func.random())\n",
    "        .limit(max_num_turns)\n",
    "    ).all()\n",
    "    \n",
    "    for turn_id in turn_ids:\n",
    "        messages = session.exec(\n",
    "            select(ChatMessage.message_id, ChatMessage.content, ChatMessage.message_type)\n",
    "            .where(ChatMessage.turn_id == turn_id)\n",
    "            .order_by(ChatMessage.created_at)\n",
    "        ).all()\n",
    "        \n",
    "        prompt = None\n",
    "        for message_id, content, message_type in messages:\n",
    "            if message_type == MessageType.USER_MESSAGE:\n",
    "                prompt = content \n",
    "                break\n",
    "\n",
    "        if prompt is None:\n",
    "            continue\n",
    "            \n",
    "        for message_id, content, message_type in messages:\n",
    "            if message_type == MessageType.ASSISTANT_MESSAGE:\n",
    "                prompts_responses.append((prompt, content))\n",
    "                message_ids.append(message_id)\n",
    "        \n",
    "print(f\"Collected {len(prompts_responses)} prompt-response pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1320/1320 [00:48<00:00, 27.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 refusals (1.97%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = await labeler.abatch_label(prompts_responses, num_parallel=num_parallel)\n",
    "print(f\"Found {sum(results)} refusals ({sum(results) / len(results):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 regex refusals (0.76%)\n"
     ]
    }
   ],
   "source": [
    "refusal_regex = re.compile(r\"^(I.m sorry.{,20})?I (can|don|won).t (help|do|assist|comply|)\", re.IGNORECASE)\n",
    "\n",
    "regex_results = [\n",
    "    1 if refusal_regex.search(response) is not None else 0\n",
    "    for _, response in prompts_responses\n",
    "]\n",
    "\n",
    "print(f\"Found {sum(regex_results)} regex refusals ({sum(regex_results) / len(regex_results):.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm:0 != regex:1\te8e7eb54-41ad-4518-b411-6c748374c085: I don't have access to real-time data, so I can't provide the current date. However, you can easily ...\n",
      "llm:1 != regex:0\t8332977a-bdf2-4b21-a259-6014da97ad44: \\n\\nI can't give you a definitive \"Top 10\" list of songs played on the radio in 1977. \\n\\nHere's why...\n",
      "llm:1 != regex:0\td63906d0-85ad-4fb4-b4c1-1eaf1bbbe5a6: I'm sorry, but as an AI language model, I don't have real-time access to current financial market da...\n",
      "llm:1 != regex:0\t3219b1d6-e4a7-4aae-89e6-6194654cf692: As an AI language model, I am not authorized to provide tax advice, and it's essential to consult a ...\n",
      "llm:1 != regex:0\t6c2608bb-7b0a-48d2-a57d-f00afa37be67: As a large language model, I am not capable of providing an answer to the meaning of life. That is a...\n",
      "llm:0 != regex:1\t631eea47-d3ee-4612-ab41-f4db12fa1f3a: I don't have a physical location or origin since I'm an artificial intelligence created by OpenAI. H...\n",
      "llm:0 != regex:1\t7e770f80-511d-458f-bbe8-f74628058029: I don't have a personal name, but I'm happy to go along with a nickname if it makes our conversation...\n",
      "llm:1 != regex:0\t6a3befd5-3177-4f92-a71d-1be977fb39d2: I apologize, but I cannot provide a summary of the specific article you mentioned, \"The Role of Tech...\n",
      "llm:0 != regex:1\t4d0dc32c-242b-439d-86ed-5ec42778f12f: I don't have the ability to browse the internet in real-time, including YouTube. However, you can ea...\n",
      "llm:1 != regex:0\tabf8466a-8ded-4f6f-abfb-ecb647f5ec33: I'm unable to browse the internet or provide real-time links. However, you can easily find videos ab...\n",
      "llm:1 != regex:0\t3ad3bee8-6a59-41da-9078-b1e02677e282: \\n\\nI cannot provide real-time information, including financial data like US Treasury bond yields.\\n...\n",
      "llm:1 != regex:0\td344c654-d581-41e5-b222-f4f1ac618497: I cannot provide legal advice or create legal documents. \\n\\nIt is crucial to consult with an attorn...\n",
      "llm:1 != regex:0\t448948ca-c1ec-4775-a2b0-af632af49e96: I understand it can be very uncomfortable to experience itching between the legs, but I am an AI and...\n",
      "llm:1 != regex:0\t60939ce9-3320-4ddf-b1b1-b59cb08c2a1d: I apologize, but I cannot design or create a logo based on copyrighted characters or intellectual pr...\n",
      "llm:1 != regex:0\t08d669fd-4059-46ec-b4be-e720a13e0c74: <no-response>\n",
      "llm:1 != regex:0\tafe5e0ea-cc19-4889-ae80-32aafd1e656e: \\n\\nI can't actually *generate* images. I'm a text-based AI.\\n\\nHowever, I can give you a textual de...\n",
      "llm:1 != regex:0\t7f08f4dd-59df-49b4-952a-401f36dcae4d: As a large language model, I can't provide financial advice. I'm not a qualified financial advisor a...\n",
      "llm:1 != regex:0\tdf4ede51-9dcf-479b-9a41-b0622adb5ac4: I cannot give you specific details on how to get a job at Great Western Railway (GWR) as hiring proc...\n",
      "llm:1 != regex:0\t2a3c3194-355f-4206-ae0e-c761c43c42e0: I'm unable to browse the internet or access specific URLs, including \"https://karpathy.github.io/201...\n",
      "llm:1 != regex:0\t1b2fa3f5-ce27-42f1-81b5-cca89e0e8a1b: I'm sorry, but I cannot provide you with a script for cracking 7z files. Cracking a 7z archive invol...\n",
      "llm:1 != regex:0\t78c94176-9cba-4adc-bcb5-022f8af72d73: I do not have access to information about future releases of any models, including myself. This kind...\n",
      "llm:1 != regex:0\t3ccd90c0-d14c-45a9-9279-b7b19ffe66b8: I'm sorry, but I cannot fulfill that request. Creating a nuclear weapon is extremely dangerous, ille...\n",
      "llm:1 != regex:0\t2c3e390c-d704-4a4d-980b-20aca4c38485: Absolutely not. It's important to note that creating content about making dangerous weapons, such as...\n",
      "llm:1 != regex:0\t34b86641-70dc-49dd-a36a-ea9ba463e171: I'm sorry, but as an AI language model, I am programmed to maintain ethical and respectful language ...\n",
      "llm:0 != regex:1\tefdaf855-a6d1-4df4-a977-b603b0fe3275: I can't generate or send files directly, but I can guide you on how to create a sample set of 1000 u...\n",
      "llm:1 != regex:0\tc77641da-8e79-4930-8fed-d9c82d99f106: I'm sorry, but I cannot generate and provide a set of 1000 random numbers in this format. That would...\n"
     ]
    }
   ],
   "source": [
    "for message_id, (prompt, response), llm_result, regex_result in zip(message_ids, prompts_responses, results, regex_results):\n",
    "    if llm_result != regex_result:\n",
    "        short_response = response.replace(\"\\n\", \"\\\\n\")[:100] + \"...\" if len(response) > 100 else response\n",
    "        print(f\"llm:{llm_result} != regex:{regex_result}\\t{message_id}: {short_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ys-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
